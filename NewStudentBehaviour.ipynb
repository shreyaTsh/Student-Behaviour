{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2a10a6-8307-4c33-8f6d-8bc6f6b0ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the path to your dataset\n",
    "Datadir = \"C:\\\\Users\\\\shrey\\\\Downloads\\\\datasets\\\\train\"\n",
    "\n",
    "# Define the list of emotion categories\n",
    "Classes = [\"surprise\",\"sad\",\"neutral\",\"happy\",\"fear\",\"disgust\",\"angry\"]  # Add more classes if needed\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_dataset():\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for category in Classes:\n",
    "        path = os.path.join(Datadir, category)\n",
    "        class_num = Classes.index(category)\n",
    "\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                img_arr = cv2.resize(img_arr, (50, 50))  # Adjust the size as needed\n",
    "                data.append(img_arr)\n",
    "                labels.append(class_num)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading image: {os.path.join(path, img)}\")\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73aa09f5-ae9e-4d3c-b28c-f6a216c460c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28680\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf681c0-1331-4076-b7a5-65fc78e1e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "717/717 [==============================] - 112s 154ms/step - loss: 1.3206 - accuracy: 0.4604 - val_loss: 8.3643 - val_accuracy: 0.0204\n",
      "Epoch 2/20\n",
      "717/717 [==============================] - 116s 162ms/step - loss: 1.1193 - accuracy: 0.5573 - val_loss: 9.9326 - val_accuracy: 0.0282\n",
      "Epoch 3/20\n",
      "717/717 [==============================] - 118s 164ms/step - loss: 1.0198 - accuracy: 0.6000 - val_loss: 9.4135 - val_accuracy: 0.0333\n",
      "Epoch 4/20\n",
      "717/717 [==============================] - 111s 154ms/step - loss: 0.9288 - accuracy: 0.6379 - val_loss: 11.4173 - val_accuracy: 0.0486\n",
      "Epoch 5/20\n",
      "717/717 [==============================] - 119s 165ms/step - loss: 0.8345 - accuracy: 0.6815 - val_loss: 12.3982 - val_accuracy: 0.0579\n",
      "Epoch 6/20\n",
      "717/717 [==============================] - 115s 160ms/step - loss: 0.7273 - accuracy: 0.7240 - val_loss: 11.9119 - val_accuracy: 0.0833\n",
      "Epoch 7/20\n",
      "717/717 [==============================] - 117s 164ms/step - loss: 0.6136 - accuracy: 0.7725 - val_loss: 14.1380 - val_accuracy: 0.0417\n",
      "Epoch 8/20\n",
      "717/717 [==============================] - 107s 149ms/step - loss: 0.4886 - accuracy: 0.8211 - val_loss: 15.0415 - val_accuracy: 0.0758\n",
      "Epoch 9/20\n",
      "717/717 [==============================] - 110s 154ms/step - loss: 0.3634 - accuracy: 0.8717 - val_loss: 17.7871 - val_accuracy: 0.0673\n",
      "Epoch 10/20\n",
      "717/717 [==============================] - 112s 156ms/step - loss: 0.2562 - accuracy: 0.9120 - val_loss: 19.9033 - val_accuracy: 0.0734\n",
      "Epoch 11/20\n",
      "717/717 [==============================] - 116s 162ms/step - loss: 0.1784 - accuracy: 0.9428 - val_loss: 21.2726 - val_accuracy: 0.1013\n",
      "Epoch 12/20\n",
      "717/717 [==============================] - 120s 167ms/step - loss: 0.1267 - accuracy: 0.9604 - val_loss: 23.5460 - val_accuracy: 0.0898\n",
      "Epoch 13/20\n",
      "717/717 [==============================] - 112s 156ms/step - loss: 0.1057 - accuracy: 0.9679 - val_loss: 27.3959 - val_accuracy: 0.0682\n",
      "Epoch 14/20\n",
      "717/717 [==============================] - 112s 157ms/step - loss: 0.0857 - accuracy: 0.9740 - val_loss: 29.1883 - val_accuracy: 0.1074\n",
      "Epoch 15/20\n",
      "717/717 [==============================] - 113s 157ms/step - loss: 0.0689 - accuracy: 0.9797 - val_loss: 28.7257 - val_accuracy: 0.0856\n",
      "Epoch 16/20\n",
      "717/717 [==============================] - 111s 155ms/step - loss: 0.0776 - accuracy: 0.9761 - val_loss: 29.8455 - val_accuracy: 0.0785\n",
      "Epoch 17/20\n",
      "717/717 [==============================] - 112s 156ms/step - loss: 0.0720 - accuracy: 0.9779 - val_loss: 31.4783 - val_accuracy: 0.0765\n",
      "Epoch 18/20\n",
      "717/717 [==============================] - 113s 157ms/step - loss: 0.0513 - accuracy: 0.9854 - val_loss: 31.5389 - val_accuracy: 0.0847\n",
      "Epoch 19/20\n",
      "717/717 [==============================] - 109s 152ms/step - loss: 0.0476 - accuracy: 0.9861 - val_loss: 31.2207 - val_accuracy: 0.0886\n",
      "Epoch 20/20\n",
      "717/717 [==============================] - 110s 153ms/step - loss: 0.0557 - accuracy: 0.9827 - val_loss: 34.6858 - val_accuracy: 0.0769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fab799a0b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data, labels = load_dataset()\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "X_train, y_train = data[:split_index], labels[:split_index]\n",
    "X_test, y_test = data[split_index:], labels[split_index:]\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(50, 50, 1)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(Classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train.reshape(-1, 50, 50, 1), y_train, epochs=20, validation_data=(X_test.reshape(-1, 50, 50, 1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ec2b1c-83ca-4496-83dc-106e7cb7b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_1.keras\")\n",
    "model= tf.keras.models.load_model(\"my_model_1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6751fcd5-3f20-4068-b632-1c02a3e9a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"my_model_1.keras\")\n",
    "\n",
    "# Load the pre-trained Haar Cascade face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Create a list of emotion labels\n",
    "emotion_labels = [\"surprise\",\"sad\",\"neutral\",\"happy\",\"fear\",\"disgust\",\"angry\"]\n",
    "\n",
    "#Video:\n",
    "# video_path =\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Desktop\\\\playing-boy-happy-boy-together-portrait.jpg\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a single frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Video ended or cannot be opened.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region\n",
    "        face_roi = gray_frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Resize the face region to match the input size of the model\n",
    "        resized_face = cv2.resize(face_roi, (50, 50))\n",
    "\n",
    "        # Normalize the pixel values to be between 0 and 1\n",
    "        normalized_face = resized_face / 255.0\n",
    "\n",
    "        # Reshape the face to match the input shape expected by the model\n",
    "        input_face = normalized_face.reshape(-1, 50, 50, 1)\n",
    "\n",
    "        # Make a prediction using the trained model\n",
    "        predictions = model.predict(input_face)\n",
    "\n",
    "        # Get the index of the predicted emotion\n",
    "        predicted_emotion_index = np.argmax(predictions)\n",
    "\n",
    "        # Get the corresponding emotion label\n",
    "        facial_expression = emotion_labels[predicted_emotion_index]\n",
    "\n",
    "        # Define behaviors based on predicted emotions\n",
    "        if facial_expression in ['happy', 'neutral']:\n",
    "            behavior = 'Engaged'\n",
    "        elif facial_expression in ['sad', 'angry']:\n",
    "            behavior = 'Distressed'\n",
    "        elif facial_expression == 'surprise':\n",
    "            behavior = 'Curious'\n",
    "        elif facial_expression == 'fear':\n",
    "            behavior = 'Anxious'\n",
    "        elif facial_expression == 'disgust':\n",
    "            behavior = 'Uncomfortable'\n",
    "        else:\n",
    "            behavior = 'Uncertain' \n",
    "        \n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "# Main loop to capture video frames and c\n",
    "        # Display the predicted emotion and behavior on the frame\n",
    "        cv2.putText(frame, f\"Emotion: {facial_expression}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, f\"Behavior: {behavior}\", (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw a rectangle around the detected face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Facial Expression Recognition', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video file and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de372a8-010f-4fbd-9291-63e3809fccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n",
      "The predicted emotion is: surprise\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Replace 'your_model_path' with the actual path to your pre-trained model\n",
    "model = load_model('my_model_1.keras')\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(50, 50), color_mode='grayscale')\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "    return img_array\n",
    "\n",
    "def predict_emotion(model, img_array):\n",
    "    prediction = model.predict(img_array)\n",
    "    emotion_labels = [\"surprise\",\"sad\",\"neutral\",\"happy\",\"fear\",\"disgust\",\"angry\"]\n",
    "    predicted_emotion = emotion_labels[np.argmax(prediction)]\n",
    "    return predicted_emotion\n",
    "# Example usage\n",
    "image_path =r\"C:\\Users\\shrey\\OneDrive\\Desktop\\project student\\cc_istock_78822855_large-2_16x9.jpg\"\n",
    "img_array = preprocess_image(image_path)\n",
    "predicted_emotion = predict_emotion(model, img_array)\n",
    "print(f'The predicted emotion is: {predicted_emotion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79807425-5b32-4ce3-b971-e0b6f6a3770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"my_model_1.keras\")\n",
    "\n",
    "# Load the pre-trained Haar Cascade face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Create a list of emotion labels\n",
    "emotion_labels = [\"surprise\",\"sad\",\"neutral\",\"happy\",\"fear\",\"disgust\",\"angry\"]\n",
    "\n",
    "# Load the image\n",
    "# image_path = \"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Desktop\\\\360_F_321726194_pHtxATrvAwNke9rwEEmUDVNXBIRJGefv.jpg\"  # Replace with your image file path\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "# Convert the frame to grayscale\n",
    "gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the frame\n",
    "faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    # Extract the face region\n",
    "    face_roi = gray_frame[y:y + h, x:x + w]\n",
    "\n",
    "    # Resize the face region to match the input size of the model\n",
    "    resized_face = cv2.resize(face_roi, (50, 50))\n",
    "\n",
    "    # Normalize the pixel values to be between 0 and 1\n",
    "    normalized_face = resized_face / 255.0\n",
    "\n",
    "    # Reshape the face to match the input shape expected by the model\n",
    "    input_face = normalized_face.reshape(-1, 50, 50, 1)\n",
    "\n",
    "    # Make a prediction using the trained model\n",
    "    predictions = model.predict(input_face)\n",
    "\n",
    "    # Get the index of the predicted emotion\n",
    "    predicted_emotion_index = np.argmax(predictions)\n",
    "\n",
    "    # Get the corresponding emotion label\n",
    "    facial_expression = emotion_labels[predicted_emotion_index]\n",
    "\n",
    "    # Define behaviors based on predicted emotions\n",
    "    if facial_expression in ['happy', 'neutral']:\n",
    "        behavior = 'Engaged'\n",
    "    elif facial_expression in ['sad', 'angry']:\n",
    "        behavior = 'Distressed'\n",
    "    elif facial_expression == 'surprise':\n",
    "        behavior = 'Curious'\n",
    "    elif facial_expression == 'fear':\n",
    "        behavior = 'Anxious'\n",
    "    elif facial_expression == 'disgust':\n",
    "        behavior = 'Uncomfortable'\n",
    "    else:\n",
    "        behavior = 'Uncertain'\n",
    "\n",
    "    # Display the predicted emotion and behavior on the frame\n",
    "    cv2.putText(frame, f\"Emotion: {facial_expression}\", (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f\"Behavior: {behavior}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "  \n",
    "\n",
    "    # Draw a rectangle around the detected face\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "# Display the frame\n",
    "cv2.imshow('Facial Expression Recognition', frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f6a6d-0589-4962-b345-af3f74b4ef6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d273fde-8558-4f84-92c6-365716b8cd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
